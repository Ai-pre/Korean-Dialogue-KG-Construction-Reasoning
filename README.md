# Korean Dialogue-based KG-aware Reasoning System

**R-GCN + ATOMIC-style Knowledge Graph for Commonsense Inference**

## ğŸ“Œ Overview

This project proposes a **KG-aware reasoning framework** for Korean daily dialogue, addressing a critical limitation of large language models (LLMs):

> **LLMs often fail to maintain consistent commonsense causal reasoning (intentâ€“effectâ€“emotion) in Korean conversational contexts.**

Instead of relying solely on parametric knowledge inside LLMs, this work introduces an **external structured commonsense Knowledge Graph (KG)** and integrates it into the inference pipeline using:

* **ATOMIC-style relational structure**
* **R-GCN (Relational Graph Convolutional Network)**
* **Event-level retrieval + prompt injection**

The system demonstrates that **KG-augmented inference produces more stable, causal-consistent, and hallucination-resistant outputs** than KG-free baselines.

---

## ğŸ” Problem Motivation

While modern LLMs generate fluent Korean text, we observed repeated failures in:

* Mixing causal directions (cause â†” effect)
* Confusing intent, emotion, and reaction types
* Making unjustified inference jumps (hallucination)
* Producing overly abstract or generic explanations

These issues are especially severe in **short, ambiguous daily dialogue**, where commonsense grounding is required.

---

## ğŸ§  Core Idea

LLMs should **not infer everything alone**.

Instead:

1. **Extract events from dialogue**
2. **Ground them in a structured commonsense KG**
3. **Use KG as a latent reasoning space**, not a direct answer source
4. **Guide LLM reasoning paths via retrieved KG context**

---

## ğŸ— System Architecture

```
User Dialogue
      â†“
Event Extraction (GPT-4o mini)
      â†“
ATOMIC-style Triple Generation (Korean)
      â†“
Knowledge Graph Construction
      â†“
R-GCN Training (Node Contextualization)
      â†“
EventMatcher (Semantic Retrieval)
      â†“
KG-aware Prompt Injection
      â†“
LLM Inference (Qwen / LLaMA)
```

---

## ğŸ§± Knowledge Graph Construction

### ğŸ”¹ Why Not Translate English ATOMIC?

We initially attempted to translate the English ATOMIC dataset, but encountered major issues:

* PersonX / PersonY templates break Korean naturalness
* Subject ambiguity and relation direction confusion
* Unstable node semantics for graph learning

â¡ï¸ **Decision**:
âŒ Drop English ATOMIC
âœ… Build **Korean-native ATOMIC-style KG from scratch**

---

### ğŸ”¹ Event & Relation Generation

Using **GPT-4o mini**, we automatically generate:

* **Event** (Korean natural sentence)
* **ATOMIC relations (9 types)**:

  * `xIntent`, `xNeed`, `xEffect`, `xReact`, `xWant`
  * `oEffect`, `oReact`, `oWant`
  * `xAttr`

Strict constraints were enforced:

* One relation = one sentence
* No moralizing / over-generalization
* No abstract norms
* Causality must be explicit

---

## ğŸ§ª KG Quality Control

Randomly sampled **500 triples** were evaluated via GPT-4o mini on:

* **Consistency**
* **Commonsense Plausibility**
* **Factuality**

| Metric      | Score          |
| ----------- | -------------- |
| Consistency | 4.00           |
| Commonsense | 4.40           |
| Factuality  | 4.00           |
| **Average** | **4.13 / 5.0** |

Low-quality triples were aggressively discarded.
â¡ï¸ **Quality > Quantity**

---

## ğŸ”— R-GCN Reasoning Module

### Purpose

R-GCN is **not** used to output answers.

Instead, it learns:

> *How events are contextually connected via relational structure*

### Design

* **Node**: Korean event sentence
* **Edge**: ATOMIC relation type
* **Output**: Context-aware event embeddings

This forms a **latent reasoning space** that helps guide LLM inference.

---

## ğŸ” EventMatcher (Retrieval Layer)

At inference time:

1. User input is embedded
2. Most semantically similar event nodes are retrieved
3. Their neighboring relations (`xIntent`, `xReact`, etc.) are collected
4. Retrieved knowledge is summarized and injected into the prompt

This prevents blind generation and anchors reasoning.

---

## ğŸ¤– KG-aware Inference Pipeline

1. Encode user input
2. Retrieve relevant KG events via EventMatcher
3. Extract relational context (subgraph)
4. Inject KG facts into LLM prompt
5. Generate grounded response

> **KG is used as contextual guidance, not as explicit facts to parrot**

---

## ğŸ§ª Experimental Strategy (KG ON / OFF)

To verify KG effectiveness, we conducted **controlled A/B testing**.

### Settings

* **Baseline (KG-Free)**
  LLM inference using only pretrained knowledge
* **Proposed (KG-Augmented)**
  Same LLM + retrieved KG context injected into prompt

### Model

* `llama-3-Korean-Bllossom-8B`

---

## ğŸ“Š Evaluation Criteria

### Qualitative Focus

* **Causal Consistency**
* **Emotionâ€“Intentâ€“Effect alignment**
* **Hallucination frequency**
* **Specificity vs abstraction**
* **Inference stability**

---

## ğŸ“ˆ Results & Analysis

### Case Study

**Input**:

> â€œë°°ê°€ ë„ˆë¬´ ì•„íŒŒì„œ ì¡°í‡´í•˜ê³  ì‹¶ì–´.â€

#### âŒ KG-Free (Baseline)

* Misinterpreted â€œë°°â€ metaphorically
* Injected unrelated school stress narratives
* Produced abstract, incoherent reasoning

â¡ï¸ **Hallucination + causal failure**

#### âœ… KG-Augmented

* Retrieved KG facts:

  * Overeating â†’ stomach pain
  * Pain â†’ desire to rest
  * Honest expression of discomfort
* Built clear causal chain:

  ```
  ê³¼ì‹ â†’ ë³µí†µ â†’ íœ´ì‹ í•„ìš” â†’ ì¡°í‡´ ì˜ë„
  ```

â¡ï¸ **Grounded, specific, causally coherent response**

---

## âœ… Conclusion

This project demonstrates that:

* LLMs alone are insufficient for stable commonsense reasoning
* **Structured KG acts as a causal anchor**
* KG-aware prompting significantly reduces hallucination
* Reasoning quality improves without modifying LLM weights

> **The key is not bigger models, but better knowledge structuring and retrieval.**

---

## ğŸ”® Future Work

* Multi-hop reasoning over KG
* Retrieval-aware selective knowledge injection
* Dynamic relation-path reasoning (`xIntent â†’ xEffect â†’ oReact`)
* Alignment-aware KG filtering

---

## ğŸ“ Repository Structure (Recommended)

```
kg_project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ korean_atomic_triples.json
â”œâ”€â”€ graph/
â”‚   â””â”€â”€ graph_data.pkl
â”œâ”€â”€ rgcn/
â”‚   â”œâ”€â”€ rgcn_model.py
â”‚   â””â”€â”€ train_rgcn.py
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ event_matcher.py
â”‚   â”œâ”€â”€ kg_on_off.py
â”‚   â””â”€â”€ prompt_builder.py
â””â”€â”€ README.md
```
